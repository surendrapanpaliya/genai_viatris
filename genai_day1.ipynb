{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e951e1",
   "metadata": {},
   "source": [
    "#Step 1: Install Required Libraries\n",
    "\n",
    "pip install openai\n",
    "pip install google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0482ac",
   "metadata": {},
   "source": [
    "pip install --upgrade openai python-dotenv\n",
    "•\topenai is the official Python client.  \n",
    "•\tpython-dotenv helps load OPENAI_API_KEY from a .env file (cleaner than hard-coding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7636b7",
   "metadata": {},
   "source": [
    "Create .env file\n",
    "\n",
    "OPENAI_API_KEY=\"Your API KEY\"\n",
    "\n",
    "2) Set your API key (safely)\n",
    "Create a .env file (same folder as your script):\n",
    "OPENAI_API_KEY=sk-...your key...\n",
    "Then load it in Python (next step). OpenAI recommends environment variables for key safety.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f3d3a",
   "metadata": {},
   "source": [
    "3) Initialize the client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd2ed2d",
   "metadata": {},
   "source": [
    "4) Make your first chat call (GPT-5)\n",
    "\n",
    "Use the model name your account is provisioned \n",
    "\n",
    "for (e.g., \"gpt-5\" or a specific snapshot you see in the dashboard).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ec449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4911c051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Decouple deploy from release: Ship code to production continuously, but expose features to users only when ready.\n",
      "- Progressive delivery: Gradually roll out by percentage, cohort, region, or account to reduce blast radius and validate in real traffic.\n",
      "- Fast, safe rollback: Use flags as kill switches to disable problematic code instantly without a redeploy, lowering MTTR.\n",
      "- Trunk-based development: Merge small, incomplete slices behind flags to avoid long-lived branches and big-bang releases.\n",
      "- Experimentation and A/B testing: Run controlled experiments and collect impact metrics before full rollout.\n",
      "- Environment and customer targeting: Enable per-env, per-tenant, or per-license gating without rebuilding artifacts.\n",
      "- Operational resilience: Toggle heavy or risky paths during incidents, maintenance windows, or peak load.\n",
      "- Observability-driven guardrails: Integrate flags with metrics/error rates for automated halts or rollbacks.\n",
      "- Safer migrations: Orchestrate schema flips, dual reads/writes, and dependency swaps incrementally.\n",
      "- Product-control of release: Empower PM/QA/Support to schedule and manage releases independently of engineering deploy cycles.\n",
      "- Better DORA outcomes: Higher deployment frequency and lower change failure rate through smaller, reversible changes.\n",
      "\n",
      "Caveat: requires disciplined flag lifecycle (ownership, expiry/cleanup), governance, and tooling to avoid tech debt and complexity.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # pulls OPENAI_API_KEY from .env\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "resp = client.responses.create(\n",
    "    model=\"gpt-5\",  # replace with your provisioned GPT-5 model\n",
    "    input=\"You are a concise senior developer assistant.Summarize why teams adopt feature flags in CI/CD\")\n",
    "\n",
    "print(resp.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422fa0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under a silver moon, a sleepy unicorn tiptoed through a whispering meadow, sprinkling stardust over every child’s dreams until the whole world sighed goodnight.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # pulls OPENAI_API_KEY from .env\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76880ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aye, mostly optional, matey! JavaScript’s Automatic Semicolon Insertion (ASI) drops semicolons fer ye at many line breaks and before closing braces. But there be treacherous reefs where ye should plant a semicolon yerself, else ye’ll scuttle the ship:\n",
      "\n",
      "- When puttin’ two statements on the same line.\n",
      "- When a new line starts with tokens that can “attach” to the previous line:\n",
      "  - ( or [  — could be seen as a call or indexing of the prior expression\n",
      "  - ` (template literal) — might become a tagged template on the prior identifier\n",
      "  - + or - — could be parsed as a unary operator on the prior expression\n",
      "  - / — might be parsed as division instead of a regex literal\n",
      "  - . or ?. — property access/optional chaining on the prior expression\n",
      "- After return, throw, break, or continue if ye put a newline right after the keyword:\n",
      "  - return\n",
      "    { a: 1 } // returns undefined because ASI inserts a semicolon after return\n",
      "- When concatenatin’ files, put a defensive leading semicolon at the start of a file that begins with one of the risky starters above.\n",
      "\n",
      "Rule o’ thumb: Semicolons be optional, but if a new line could be read as continuin’ the previous one, either add a semicolon at the end of the previous line or a leading semicolon at the start of the risky line. Pick a style (always or mostly-none) and stick to it, arrr!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    reasoning={\"effort\": \"low\"},\n",
    "    instructions=\"Talk like a pirate.\",\n",
    "    input=\"Are semicolons optional in JavaScript?\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f2f98c",
   "metadata": {},
   "source": [
    "## Tokenization — How Text Becomes Numbers\n",
    "\n",
    "    Why tokens matter:\n",
    "\n",
    "    LLMs don’t understand words—they understand numbers.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    Text:\n",
    "    “Patient has fever and headache.”\n",
    "    When converted into tokens (pieces of words):\n",
    "    [\"Patient\", \" has\", \" fever\", \" and\", \" headache\", \".\"]\n",
    "\n",
    "    More tokens = More cost + More time to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ab532b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Patient has fever and headache.\n",
      "Number of tokens: 6\n",
      "Tokens: [37692, 706, 34653, 323, 47846, 13]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "text = \"Patient has fever and headache.\"\n",
    "print(\"Text:\", text)\n",
    "tokens = enc.encode(text)\n",
    "print(\"Number of tokens:\", len(tokens))\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55762a72",
   "metadata": {},
   "source": [
    "Try this:\n",
    "•\tAdd extra words like “The patient who came yesterday…”\n",
    "•\tNotice how the number of tokens increases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5be0e7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Patient has fever and headache. The Patient was prescribed 500mg of Paracetamol twice daily.\n",
      "Number of tokens: 21\n",
      "Tokens: [37692, 706, 34653, 323, 47846, 13, 578, 30024, 574, 32031, 220, 2636, 12591, 315, 4366, 68323, 309, 337, 11157, 7446, 13]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "text = \"Patient has fever and headache. The Patient was prescribed 500mg of Paracetamol twice daily.\"\n",
    "print(\"Text:\", text)\n",
    "tokens = enc.encode(text)\n",
    "print(\"Number of tokens:\", len(tokens))\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e711493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai tiktoken numpy\n",
    "\n",
    "#import os\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # or set in your shell\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Choose models you have access to\n",
    "CHAT_MODEL = \"gpt-5-chat\" # ← replace with your org’s GPT-5 chat model name\n",
    "\n",
    "EMBED_MODEL = \"text-embedding-3-large\" # robust general-purpose embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894e4e10",
   "metadata": {},
   "source": [
    "## A) Tokenization & token counting\n",
    "\n",
    "    Why: estimate cost/fit and prevent overflows before you call the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa2565d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 14\n"
     ]
    }
   ],
   "source": [
    "# Choose an encoding close to your chat model (cl100k_base works for GPT-4/5 family)\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "sample = \"Build a weekly status report with risks, blockers, and next steps.\"\n",
    "\n",
    "print(\"Tokens:\", count_tokens(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02640caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.378 :: Blocker: Dependency on vendor SSO integration pending approval.\n",
      "0.371 :: Next steps: Load test, error budget policy, and rollback plan.\n",
      "0.313 :: Risk: Payment gateway intermittently times out in peak hours.\n"
     ]
    }
   ],
   "source": [
    "def embed_texts(texts):\n",
    "    \"\"\"\n",
    "    Returns a 2D numpy array of embeddings, one row per text.\n",
    "    \"\"\"\n",
    "    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "    return np.array([d.embedding for d in resp.data])\n",
    "\n",
    "def cosine_sim_matrix(A, B):\n",
    "    # A: (m, d), B: (n, d)\n",
    "    A_norm = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-12)\n",
    "    B_norm = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-12)\n",
    "    return A_norm @ B_norm.T\n",
    "\n",
    "# Example knowledge base (could be Jira tickets, sprint notes, SOPs)\n",
    "kb_chunks = [\n",
    "    \"Sprint 34: Completed API refactor; performance +22%.\",\n",
    "    \"Risk: Payment gateway intermittently times out in peak hours.\",\n",
    "    \"Blocker: Dependency on vendor SSO integration pending approval.\",\n",
    "    \"Next steps: Load test, error budget policy, and rollback plan.\"\n",
    "]\n",
    "\n",
    "# Pre-compute KB embeddings once (cache in your app)\n",
    "kb_vecs = embed_texts(kb_chunks)\n",
    "\n",
    "# Query → get top-K similar chunks\n",
    "def retrieve(query, top_k=3):\n",
    "    q_vec = embed_texts([query])  # (1, d)\n",
    "    sims = cosine_sim_matrix(q_vec, kb_vecs).ravel()\n",
    "    idx = np.argsort(-sims)[:top_k]\n",
    "    return [kb_chunks[i] for i in idx], sims[idx]\n",
    "\n",
    "q = \"What are the current risks and blockers for the release?\"\n",
    "ctx, scores = retrieve(q, top_k=3)\n",
    "for c, s in zip(ctx, scores):\n",
    "    print(f\"{s:.3f} :: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e7629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple budgeter for chat messages (system + context + user)\n",
    "# You can extend this with: role weights, summary fallbacks, or tool-call reserves.\n",
    "\n",
    "def build_chat_messages(system_msg, user_msg, context_blocks, max_prompt_tokens=12000):\n",
    "    \"\"\"\n",
    "    Returns a pruned message list that fits within max_prompt_tokens.\n",
    "    Note: reserve room for the model's output separately (e.g., 1–2k tokens).\n",
    "    \"\"\"\n",
    "    def tcount_msgs(msgs):\n",
    "        # crude count: concatenate with role tags\n",
    "        joined = \"\\n\".join(f\"{m['role'].upper()}: {m['content']}\" for m in msgs)\n",
    "        return count_tokens(joined)\n",
    "\n",
    "    msgs = [{\"role\": \"system\", \"content\": system_msg}]\n",
    "    # Add context blocks from most to least relevant—assumes they came sorted by similarity\n",
    "    for block in context_blocks:\n",
    "        trial = msgs + [{\"role\": \"system\", \"content\": f\"[CONTEXT]\\n{block}\"}]\n",
    "        if tcount_msgs(trial) <= max_prompt_tokens:\n",
    "            msgs = trial\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # finally add user prompt if it fits; else truncate user content conservatively\n",
    "    trial = msgs + [{\"role\": \"user\", \"content\": user_msg}]\n",
    "    if tcount_msgs(trial) <= max_prompt_tokens:\n",
    "        return trial\n",
    "\n",
    "    # Truncate user message (simple strategy—replace with your tokenizer-aware truncation)\n",
    "    user_short = user_msg[:2000]\n",
    "    trial = msgs + [{\"role\": \"user\", \"content\": user_short}]\n",
    "    return trial\n",
    "\n",
    "system = \"You are a project co-pilot. Be concise, structured, and factual.\"\n",
    "user = f\"Create a weekly release status report.\\nQuestion: {q}\\nInclude: risks, blockers, next steps.\"\n",
    "\n",
    "messages = build_chat_messages(system, user, ctx, max_prompt_tokens=12000)\n",
    "\n",
    "print(\"Prompt tokens (approx):\", count_tokens(\"\\n\".join(m[\"content\"] for m in messages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "491a5604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OpenAI in module openai:\n",
      "\n",
      "class OpenAI(openai._base_client.SyncAPIClient)\n",
      " |  OpenAI(*, api_key: 'str | None' = None, organization: 'str | None' = None, project: 'str | None' = None, base_url: 'str | httpx.URL | None' = None, websocket_base_url: 'str | httpx.URL | None' = None, timeout: 'Union[float, Timeout, None, NotGiven]' = NOT_GIVEN, max_retries: 'int' = 2, default_headers: 'Mapping[str, str] | None' = None, default_query: 'Mapping[str, object] | None' = None, http_client: 'httpx.Client | None' = None, _strict_response_validation: 'bool' = False) -> 'None'\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OpenAI\n",
      " |      openai._base_client.SyncAPIClient\n",
      " |      openai._base_client.BaseClient\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, api_key: 'str | None' = None, organization: 'str | None' = None, project: 'str | None' = None, base_url: 'str | httpx.URL | None' = None, websocket_base_url: 'str | httpx.URL | None' = None, timeout: 'Union[float, Timeout, None, NotGiven]' = NOT_GIVEN, max_retries: 'int' = 2, default_headers: 'Mapping[str, str] | None' = None, default_query: 'Mapping[str, object] | None' = None, http_client: 'httpx.Client | None' = None, _strict_response_validation: 'bool' = False) -> 'None'\n",
      " |      Construct a new synchronous OpenAI client instance.\n",
      " |      \n",
      " |      This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n",
      " |      - `api_key` from `OPENAI_API_KEY`\n",
      " |      - `organization` from `OPENAI_ORG_ID`\n",
      " |      - `project` from `OPENAI_PROJECT_ID`\n",
      " |  \n",
      " |  audio = <functools.cached_property object>\n",
      " |  batches = <functools.cached_property object>\n",
      " |  beta = <functools.cached_property object>\n",
      " |  chat = <functools.cached_property object>\n",
      " |  completions = <functools.cached_property object>\n",
      " |  containers = <functools.cached_property object>\n",
      " |  copy(self, *, api_key: 'str | None' = None, organization: 'str | None' = None, project: 'str | None' = None, websocket_base_url: 'str | httpx.URL | None' = None, base_url: 'str | httpx.URL | None' = None, timeout: 'float | Timeout | None | NotGiven' = NOT_GIVEN, http_client: 'httpx.Client | None' = None, max_retries: 'int | NotGiven' = NOT_GIVEN, default_headers: 'Mapping[str, str] | None' = None, set_default_headers: 'Mapping[str, str] | None' = None, default_query: 'Mapping[str, object] | None' = None, set_default_query: 'Mapping[str, object] | None' = None, _extra_kwargs: 'Mapping[str, Any]' = {}) -> 'Self'\n",
      " |      Create a new client instance re-using the same options given to the current client with optional overriding.\n",
      " |  \n",
      " |  embeddings = <functools.cached_property object>\n",
      " |  evals = <functools.cached_property object>\n",
      " |  files = <functools.cached_property object>\n",
      " |  fine_tuning = <functools.cached_property object>\n",
      " |  images = <functools.cached_property object>\n",
      " |  models = <functools.cached_property object>\n",
      " |  moderations = <functools.cached_property object>\n",
      " |  responses = <functools.cached_property object>\n",
      " |  uploads = <functools.cached_property object>\n",
      " |  vector_stores = <functools.cached_property object>\n",
      " |  with_options = copy(self, *, api_key: 'str | None' = None, organization: 'str | None' = None, project: 'str | None' = None, websocket_base_url: 'str | httpx.URL | None' = None, base_url: 'str | httpx.URL | None' = None, timeout: 'float | Timeout | None | NotGiven' = NOT_GIVEN, http_client: 'httpx.Client | None' = None, max_retries: 'int | NotGiven' = NOT_GIVEN, default_headers: 'Mapping[str, str] | None' = None, set_default_headers: 'Mapping[str, str] | None' = None, default_query: 'Mapping[str, object] | None' = None, set_default_query: 'Mapping[str, object] | None' = None, _extra_kwargs: 'Mapping[str, Any]' = {}) -> 'Self'\n",
      " |  \n",
      " |  with_raw_response = <functools.cached_property object>\n",
      " |  with_streaming_response = <functools.cached_property object>\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  auth_headers\n",
      " |  \n",
      " |  default_headers\n",
      " |  \n",
      " |  qs\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'api_key': 'str', 'organization': 'str | None', 'pr...\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from openai._base_client.SyncAPIClient:\n",
      " |  \n",
      " |  __enter__(self: '_T') -> '_T'\n",
      " |  \n",
      " |  __exit__(self, exc_type: 'type[BaseException] | None', exc: 'BaseException | None', exc_tb: 'TracebackType | None') -> 'None'\n",
      " |  \n",
      " |  close(self) -> 'None'\n",
      " |      Close the underlying HTTPX client.\n",
      " |      \n",
      " |      The client will *not* be usable after this.\n",
      " |  \n",
      " |  delete(self, path: 'str', *, cast_to: 'Type[ResponseT]', body: 'Body | None' = None, options: 'RequestOptions' = {}) -> 'ResponseT'\n",
      " |  \n",
      " |  get(self, path: 'str', *, cast_to: 'Type[ResponseT]', options: 'RequestOptions' = {}, stream: 'bool' = False, stream_cls: 'type[_StreamT] | None' = None) -> 'ResponseT | _StreamT'\n",
      " |  \n",
      " |  get_api_list(self, path: 'str', *, model: 'Type[object]', page: 'Type[SyncPageT]', body: 'Body | None' = None, options: 'RequestOptions' = {}, method: 'str' = 'get') -> 'SyncPageT'\n",
      " |  \n",
      " |  is_closed(self) -> 'bool'\n",
      " |  \n",
      " |  patch(self, path: 'str', *, cast_to: 'Type[ResponseT]', body: 'Body | None' = None, options: 'RequestOptions' = {}) -> 'ResponseT'\n",
      " |  \n",
      " |  post(self, path: 'str', *, cast_to: 'Type[ResponseT]', body: 'Body | None' = None, options: 'RequestOptions' = {}, files: 'RequestFiles | None' = None, stream: 'bool' = False, stream_cls: 'type[_StreamT] | None' = None) -> 'ResponseT | _StreamT'\n",
      " |  \n",
      " |  put(self, path: 'str', *, cast_to: 'Type[ResponseT]', body: 'Body | None' = None, files: 'RequestFiles | None' = None, options: 'RequestOptions' = {}) -> 'ResponseT'\n",
      " |  \n",
      " |  request(self, cast_to: 'Type[ResponseT]', options: 'FinalRequestOptions', *, stream: 'bool' = False, stream_cls: 'type[_StreamT] | None' = None) -> 'ResponseT | _StreamT'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from openai._base_client.SyncAPIClient:\n",
      " |  \n",
      " |  __orig_bases__ = (openai._base_client.BaseClient[httpx.Client, openai....\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from openai._base_client.BaseClient:\n",
      " |  \n",
      " |  platform_headers(self) -> 'Dict[str, str]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from openai._base_client.BaseClient:\n",
      " |  \n",
      " |  custom_auth\n",
      " |  \n",
      " |  default_query\n",
      " |  \n",
      " |  user_agent\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from openai._base_client.BaseClient:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  base_url\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |      Parameterizes a generic class.\n",
      " |      \n",
      " |      At least, parameterizing a generic class is the *main* thing this method\n",
      " |      does. For example, for some generic class `Foo`, this is called when we\n",
      " |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |      \n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "help(OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ce09b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c2588e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! \n",
      "\n",
      "First, let's calculate \\(5 \\times 5\\):\n",
      "\n",
      "\\[ 5 \\times 5 = 25 \\]\n",
      "\n",
      "Next, let's solve the addition problem \\(5 + 10 + 20 + 30\\):\n",
      "\n",
      "\\[ 5 + 10 = 15 \\]\n",
      "\\[ 15 + 20 = 35 \\]\n",
      "\\[ 35 + 30 = 65 \\]\n",
      "\n",
      "So, the sum of \\(5 + 10 + 20 + 30\\) is \\(65\\).\n",
      "\n",
      "Therefore, \\(5 \\times 5 = 25\\) and \\(5 + 10 + 20 + 30 = 65\\).\n"
     ]
    }
   ],
   "source": [
    "query = '''What is 5*5? Also solve 5 + 10 + 20 + 30 =?'''\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",  # Specify the model you want to use\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Help me with my math homework!\"}, # <-- This is the system message that provides context to the model\n",
    "    {\"role\": \"user\", \"content\": query},  # <-- This is the user message for which the model will generate a response\n",
    "  ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8410cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5*5 = 25\n",
      "5 + 10 + 20 + 30 = 65\n"
     ]
    }
   ],
   "source": [
    "query = '''What is 5*5? Also solve 5 + 10 + 20 + 30 =?'''\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-5\",  # Specify the model you want to use\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Help me with my math homework!\"}, # <-- This is the system message that provides context to the model\n",
    "    {\"role\": \"user\", \"content\": query},  # <-- This is the user message for which the model will generate a response\n",
    "  ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94837ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Decouple deploy from release: ship code continuously, expose features later via flags.\n",
      "- Safer rollouts: canary/percentage rollouts to limit blast radius and validate in production.\n",
      "- Instant rollback/kill switches: disable a bad feature without redeploying.\n",
      "- Faster velocity: merge incomplete work behind flags, enabling trunk‑based development and fewer long‑lived branches.\n",
      "- Test in production: enable for internal users or small cohorts to catch real‑world issues early.\n",
      "- Targeted releases: turn features on per user, account, region, or environment.\n",
      "- Experimentation: A/B and multivariate tests driven by flags with metrics.\n",
      "- Operational resilience: degrade or disable noncritical paths when dependencies fail.\n",
      "- Coordinated changes: manage multi‑service/multi‑step migrations and phased rollouts.\n",
      "- Compliance and control: scheduled releases, approvals, and audit trails without code changes.\n",
      "- Reduced downtime: avoid risky big‑bang releases and hotfix redeploys.\n",
      "- Better observability: correlate feature exposure with errors, performance, and business impact.\n",
      "\n",
      "Note: flags add maintenance overhead and require governance (ownership, expiration, cleanup).\n"
     ]
    }
   ],
   "source": [
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-5\",  # replace with your provisioned GPT-5 model\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise senior developer assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Summarize why teams adopt feature flags in CI/CD.\"},\n",
    "    ],\n",
    ")\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e0fceda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit breaker pattern: a guard around remote calls that fails fast when a dependency is unhealthy, preventing cascades and giving it time to recover.\n",
      "\n",
      "Core state machine\n",
      "- Closed: Calls flow normally; failures are counted.\n",
      "- Open: Calls are short-circuited immediately (fast fail) for a cool-down period.\n",
      "- Half-open: Allow a few probe calls; on success, close; on failure, open again.\n",
      "\n",
      "Key triggers and settings\n",
      "- Failure-rate threshold over a sliding window (e.g., >50% of last N calls fail).\n",
      "- Slow-call rate threshold (treat very slow calls as failures).\n",
      "- Minimum number of calls before tripping (avoid tripping on tiny samples).\n",
      "- Open duration (cool-down), then transition to half-open.\n",
      "- Permitted calls in half-open (to avoid a thundering herd).\n",
      "- Exception types to record or ignore.\n",
      "- Timeouts are essential; a call that never times out can’t be judged.\n",
      "\n",
      "How to use it\n",
      "- Wrap every outbound dependency call (per endpoint/operation) with its own breaker.\n",
      "- Combine with:\n",
      "  - Timeouts (short, per call).\n",
      "  - Limited retries with backoff and jitter (only for idempotent ops).\n",
      "  - Bulkheads (limit concurrent calls to a dependency).\n",
      "  - Fallbacks (optional, safe degradations like cached data).\n",
      "- Typical order: timeout + small retry inside a circuit breaker; ensure retries don’t exceed breaker thresholds.\n",
      "\n",
      "What to monitor\n",
      "- Open/half-open rates, failure and slow-call rates, latency, retry counts.\n",
      "- Alert on sustained open state, rising slow-call rate, fallback usage.\n",
      "\n",
      "Pitfalls\n",
      "- Unbounded retries or long timeouts defeat the breaker.\n",
      "- Retrying non-idempotent operations causes duplicates.\n",
      "- Letting half-open admit too many probes can spike a recovering service.\n",
      "- Treating all exceptions as failures (e.g., 4xx might be client errors, not dependency health).\n",
      "- Long open durations cause unnecessary downtime; too short causes flapping.\n",
      "\n",
      "Example (Resilience4j-style)\n",
      "- failureRateThreshold: 50%\n",
      "- slowCallDuration: 2s, slowCallRateThreshold: 50%\n",
      "- slidingWindow: 100 calls (or time-based)\n",
      "- waitDurationInOpenState: 30s\n",
      "- permittedCallsInHalfOpenState: 10\n",
      "- recordExceptions: IOException, TimeoutException; ignore: 4xx\n",
      "\n",
      "Ecosystem\n",
      "- Java: Resilience4j, Spring Cloud Circuit Breaker (Hystrix is deprecated).\n",
      "- .NET: Polly.\n",
      "- Node.js: opossum.\n",
      "- Service mesh: Envoy/Istio “outlier detection” provides breaker-like ejection at the network layer; still use app-level breakers for business-aware decisions.\n",
      "\n",
      "Goal: fail fast, shed load, degrade gracefully, and recover smoothly.\n"
     ]
    }
   ],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are terse.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain circuit breakers in microservices.\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(stream.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b71143c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('id', 'resp_68bd656c68148195bca3fd496285614602c50422676c9ffd')\n",
      "('created_at', 1757242732.0)\n",
      "('error', None)\n",
      "('incomplete_details', None)\n",
      "('instructions', None)\n",
      "('metadata', {})\n",
      "('model', 'gpt-5-mini-2025-08-07')\n",
      "('object', 'response')\n",
      "('output', [ResponseReasoningItem(id='rs_68bd656d25cc819591b81eb139c71d9602c50422676c9ffd', summary=[], type='reasoning', encrypted_content=None, status=None), ResponseOutputMessage(id='msg_68bd657128dc81959caea0238b56a97c02c50422676c9ffd', content=[ResponseOutputText(annotations=[], text='double bubble bath double bubble bath double bubble bath double bubble bath double bubble bath double bubble bath double bubble bath double bubble bath double bubble bath double bubble bath', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')])\n",
      "('parallel_tool_calls', True)\n",
      "('temperature', 1.0)\n",
      "('tool_choice', 'auto')\n",
      "('tools', [])\n",
      "('top_p', 1.0)\n",
      "('background', False)\n",
      "('max_output_tokens', None)\n",
      "('previous_response_id', None)\n",
      "('prompt', None)\n",
      "('reasoning', Reasoning(effort='medium', generate_summary=None, summary=None))\n",
      "('service_tier', 'default')\n",
      "('status', 'completed')\n",
      "('text', ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'))\n",
      "('truncation', 'disabled')\n",
      "('usage', ResponseUsage(input_tokens=16, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=292, output_tokens_details=OutputTokensDetails(reasoning_tokens=256), total_tokens=308))\n",
      "('user', None)\n",
      "('max_tool_calls', None)\n",
      "('prompt_cache_key', None)\n",
      "('safety_identifier', None)\n",
      "('store', True)\n",
      "('top_logprobs', 0)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "stream = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say 'double bubble bath' ten times fast.\",\n",
    "        },\n",
    "    ],\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "for event in stream:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6320f94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlock a world of adventure for your little explorer with our colorful Kids Explorer Backpack, priced at just ₹999. Designed with both style and functionality in mind, this backpack features vibrant patterns and durable, lightweight materials perfect for school days and weekend adventures alike. With spacious compartments, it easily accommodates books, snacks, and treasures, while ergonomic straps ensure comfort for growing shoulders. Equipped with sturdy zippers and a water-resistant exterior, it promises reliability rain or shine. Ideal for children aged 5-10, this backpack is the ultimate blend of fun, practicality, and value. Let imagination soar with every new journey!\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Load and Interact with GPT (OpenAI)\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "MODEL=\"gpt-4o\"\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "# Step 3: Use the OpenAI client to generate a response\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write a short product description for a ₹999 kids backpack.\"}\n",
    "    ] )\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9300311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "#help(AzureOpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5c9dff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 12\n",
      "First 10 token IDs: [9370, 5730, 553, 279, 7720, 315, 1701, 4724, 71647, 369]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Choose encoding compatible with GPT-4 and later families\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "prompt = \"Summarize the benefits of using vector embeddings for search.\"\n",
    "tokens = enc.encode(prompt)\n",
    "print(\"Token count:\", len(tokens))\n",
    "print(\"First 10 token IDs:\", tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44699d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware\n"
     ]
    }
   ],
   "source": [
    "#from openai import OpenAI\n",
    "#client = OpenAI()\n",
    "\n",
    "instructions = \"\"\"\n",
    "You are an expert in categorizing IT support tickets. Given the support\n",
    "ticket below, categorize the request into one of \"Hardware\", \"Software\",\n",
    "or \"Other\". Respond with only one of those words.\n",
    "\"\"\"\n",
    "\n",
    "ticket = \"My monitor won't turn on - help!\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\"role\": \"developer\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": ticket},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c8747a",
   "metadata": {},
   "source": [
    "1) Instruction Q&A (system / user roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b84a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector databases are specialized databases designed to handle and manage data that is represented as vectors. In the context of enterprises, these databases are particularly useful for dealing with complex data types such as images, audio, text, and other unstructured data that traditional databases struggle to process efficiently.\n",
      "\n",
      "Here's a simple breakdown of how vector databases work and their benefits for enterprises:\n",
      "\n",
      "1. **Data Representation**: In a vector database, data is represented as vectors, which are essentially arrays of numbers. These vectors capture the essential features of the data. For example, a vector might represent the characteristics of an image or the semantic meaning of a piece of text.\n",
      "\n",
      "2. **Similarity Search**: One of the primary functions of vector databases is to perform similarity searches. This means they can quickly find and retrieve data that is similar to a given query vector. This is particularly useful for applications like recommendation systems, image recognition, and natural language processing.\n",
      "\n",
      "3. **Scalability**: Vector databases are designed to handle large volumes of data efficiently. They use advanced indexing techniques to ensure that searches are fast, even as the amount of data grows.\n",
      "\n",
      "4. **Integration with AI and Machine Learning**: Vector databases are often used in conjunction with AI and machine learning models. These models generate the vectors that are stored in the database, enabling sophisticated data analysis and insights.\n",
      "\n",
      "5. **Real-time Processing**: Many vector databases support real-time data processing, which is crucial for applications that require immediate responses, such as fraud detection or\n"
     ]
    }
   ],
   "source": [
    "#GPT-4\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=300,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a precise technical explainer.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain vector databases for enterprise in simple terms.\"}\n",
    "    ],\n",
    ")\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "385ba66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short version: A vector database stores “meaning” as numbers so you can find similar things fast. It turns text, images, audio, or tables into vectors (lists of numbers). Similar items end up near each other in this high‑dimensional space. The database then finds nearest neighbors quickly, often in milliseconds, even among millions or billions of items.\n",
      "\n",
      "Why enterprises care\n",
      "- Make messy data searchable by meaning, not exact words (semantic search).\n",
      "- Power RAG (retrieve-augment-generate) for more accurate LLM answers.\n",
      "- Recommendations and personalization across products, content, or tickets.\n",
      "- Detect duplicates, near-duplicates, anomalies, or fraud patterns.\n",
      "- Cross‑modal search (e.g., search images with text).\n",
      "\n",
      "How it works (simple flow)\n",
      "1) Embed: An embedding model converts each item (document chunk, product, image) into a vector, typically 384–3072 dimensions.\n",
      "2) Store: Save the vector plus metadata (title, permissions, timestamps) as a record.\n",
      "3) Index: Build a specialized index for fast “nearest neighbor” search (ANN).\n",
      "4) Query: Convert the user query into a vector and search for nearest neighbors.\n",
      "5) Filter and re-rank: Apply metadata filters (tenant, region, date) and optionally re-rank with a stronger model or an LLM before using results.\n",
      "\n",
      "How it’s different from a traditional database or keyword search\n",
      "- Finds by meaning, not exact tokens. “How do I reset my laptop?” matches “factory restore procedure.”\n",
      "- Uses approximate nearest neighbor (ANN) indexes (like HNSW, IVF, PQ) to trade tiny accuracy for big speed.\n",
      "- Optimized for vector similarity (cosine, dot product, L2) plus metadata filters; less about joins and transactions.\n",
      "\n",
      "Core pieces you’ll choose\n",
      "- Embedding model: General vs domain-specific, multilingual needs, model versioning. Keep versions stable; re-embed if you switch models.\n",
      "- Index type:\n",
      "  - HNSW: great recall, low-latency, supports online inserts; more memory.\n",
      "  - IVF/IVF+PQ: scalable with compression; needs training step; good for very large datasets.\n",
      "  - GPU (FAISS/RAFT): very fast at high scale; consider cost and ops.\n",
      "- Distance metric: cosine or dot product for text; L2 for some vision/time-series.\n",
      "- Metadata filtering: Attribute filters (tenant, ACLs, type). Consider hybrid search (lexical + vector) for precision.\n",
      "- Consistency and updates: Freshness requirements (seconds vs minutes), delete semantics (“right to be forgotten”) and background compaction.\n",
      "- Security and governance: Row/attribute-level access, tenant isolation, encryption, audit, data residency, PII handling.\n",
      "- Cost and sizing: Memory/disk footprint, compression, throughput requirements.\n",
      "\n",
      "RAG in practice (typical enterprise pattern)\n",
      "- Chunk content (e.g., 200–500 tokens, overlap 10–20%), embed each chunk, store vector + source + ACLs.\n",
      "- At query time, embed the question, do vector search with filters, optionally re-rank with a cross-encoder, then send the top passages to the LLM with citations.\n",
      "- Cache frequent queries and results; monitor answer quality and hallucinations.\n",
      "\n",
      "Rules of thumb for sizing and performance\n",
      "- Raw vector size ≈ dimensions × 4 bytes (float32). Example: 1536-d ≈ 6 KB per vector. Index overhead can be 1.5–3× for HNSW.\n",
      "- 10 million vectors at 6 KB raw ≈ 60 GB raw; with HNSW overhead, 100–180 GB memory. Use PQ or scalar quantization to reduce 4–10× with some recall loss.\n",
      "- Latency: Single-digit to low tens of ms for top‑k=10 at 1–10M scale on a few CPU/GPU nodes; p95 depends on filters and network.\n",
      "- Quality: Target recall@k ≥ 0.9 for your use case. Track nDCG and business metrics (clicks, resolution time).\n",
      "\n",
      "Operational concerns\n",
      "- Ingestion: Batch vs streaming; dedup; document splitting; language detection; PII redaction.\n",
      "- Index maintenance: Background builds, compaction, snapshot/restore, re-index when model changes.\n",
      "- Deletions and “right to be forgotten”: Ensure tombstones propagate and data is actually purged from indexes and backups per policy.\n",
      "- Monitoring: Index size, memory, CPU/GPU, p50/p95 latency, QPS, recall, filter selectivity, error rates.\n",
      "- Multi-tenancy: Hard partitioning per tenant for strong isolation, or soft isolation with strict filters and ABAC; beware filter-first vs score-then-filter pitfalls.\n",
      "- Hybrid search: Combine BM25 (keywords) and vectors; re-rank top N with a cross-encoder for precision-critical queries (legal, healthcare).\n",
      "\n",
      "Where to run it\n",
      "- Dedicated vector databases (e.g., commercial managed services and open-source engines) for large scale and low latency with rich filtering.\n",
      "- Search engines with vector support (e.g., enterprise search platforms) for strong text search + semantic hybrid.\n",
      "- Relational DBs with vector extensions (e.g., PostgreSQL with pgvector, cloud variants) for moderate scale and simpler ops when you already live in SQL.\n",
      "- Choose managed if you want SLAs and reduced ops; self-host if data gravity, cost, or control dominate.\n",
      "\n",
      "When to use vs not use\n",
      "- Use when: you need semantic search across large, diverse data; recommendations; RAG; anomaly/duplicate detection.\n",
      "- Avoid or complement with keyword/SQL when: corpus is tiny; you require exact matching and complex joins; precision must be perfect; heavy transactional semantics are needed.\n",
      "\n",
      "Common pitfalls and how to avoid them\n",
      "- Embedding drift: Changing models silently degrades search. Version vectors and re-embed in batches with A/B validation.\n",
      "- Poor chunking: Too big hurts recall; too small loses context. Experiment and measure.\n",
      "- Over-filtering: Aggressive ACL/date filters can kill recall; pre-partition or build per-tenant shards to keep candidate sets large.\n",
      "- Data leakage: Enforce row-level security in the vector layer and in any cache; never bypass filters during re-ranking or LLM calls.\n",
      "- Multilingual mismatch: Use multilingual embeddings or per-language indexes; detect language at ingest.\n",
      "\n",
      "Quick glossary\n",
      "- Embedding: Numeric representation of meaning.\n",
      "- Vector: The list of numbers output by the embedding model.\n",
      "- ANN index: Data structure for fast approximate nearest neighbor search (HNSW, IVF, PQ).\n",
      "- Recall: How often the true nearest neighbors are found; higher is better.\n",
      "- Hybrid search: Combining lexical and vector scores.\n",
      "\n",
      "If you share rough data size, latency, and security requirements, I can suggest a concrete architecture and index choice.\n"
     ]
    }
   ],
   "source": [
    "#GPT-5 (Responses API; use your current gpt-5 model name)\n",
    "resp = client.responses.create(\n",
    "    model=\"gpt-5\",  # replace with the exact gpt-5* model on your account\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a precise technical explainer. Verbosity: medium.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain vector databases for enterprise in simple terms.\"}\n",
    "    ]\n",
    ")\n",
    "print(resp.output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5089eb3a",
   "metadata": {},
   "source": [
    "Notes: Chat Completions uses messages=[...]. \n",
    "\n",
    "The Responses API accepts an input=[...] \n",
    "\n",
    "#array with the same roles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ef3aa",
   "metadata": {},
   "source": [
    "2) Few-shot style transfer (show, then ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2c28ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you please take a moment to fix the data pipeline? Thanks!\n"
     ]
    }
   ],
   "source": [
    "#GPT-4\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a writing coach.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Rewrite in a friendlier tone: 'Submit the report by EOD.'\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"Could you please send the report by the end of the day? Thanks!\"},\n",
    "  {\"role\": \"user\", \"content\": \"Rewrite in the same friendly tone: 'Fix the data pipeline now.'\"}\n",
    "]\n",
    "\n",
    "resp = client.chat.completions.create(model=\"gpt-4o\", temperature=0.7, messages=messages)\n",
    "\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b03a4f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you please fix the data pipeline as soon as you can? Thanks!\n"
     ]
    }
   ],
   "source": [
    "#GPT-5\n",
    "resp = client.responses.create(\n",
    "  model=\"gpt-5\",\n",
    "  input=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a writing coach. Match the tone of assistant examples. Verbosity: low.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Rewrite in a friendlier tone: 'Submit the report by EOD.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Could you please send the report by the end of the day? Thanks!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Rewrite in the same friendly tone: 'Fix the data pipeline now.'\"}\n",
    "  ]\n",
    ")\n",
    "print(resp.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cbf071",
   "metadata": {},
   "source": [
    "3) “Verbosity” / tone control (semantics, not just length)\n",
    "You can guide verbosity and tone declaratively in your system instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91a52666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Top-p Sampling (Nucleus Sampling):**\n",
      "- **Pros:** Dynamic control by adjusting samples, offers fine-tuned creativity by selecting from the most probable results.\n",
      "- **Cons:** May compromise coherence if 'p' value is too low, potentially omits diverse word options.\n",
      "\n",
      "**Temperature Sampling:**\n",
      "- **Pros:** Simple implementation; allows control over randomness by scaling probabilities, useful for consistent outputs.\n",
      "- **Cons:** High value can lead to incoherent results, while low value limits creativity by focusing on high-probability words.\n"
     ]
    }
   ],
   "source": [
    "#GPT-4\n",
    "resp = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\"role\":\"system\",\"content\":\"Be concise (2–3 bullets). Tone: professional.\"},\n",
    "    {\"role\":\"user\",\"content\":\"Summarize the pros and cons of Top-p vs Temperature.\"}\n",
    "  ],\n",
    "  max_tokens=200\n",
    ")\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98103f98",
   "metadata": {},
   "source": [
    "Tip: In GPT-5, treat “verbosity” and “tone” as semantic dials expressed in instructions. (They complement, rather than replace, hard caps like max_output_tokens.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "477fc43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Temperature: Pros—smooth, single knob; preserves token ranking; works consistently across contexts. Cons—non-adaptive; high values admit very low-probability tokens (instability); low values can make outputs overconfident/repetitive.\n",
      "- Top-p (nucleus): Pros—adaptive to entropy; trims the unlikely tail, reducing bizarre tokens; maintains fluency in high-uncertainty settings. Cons—can cut out useful rare tokens; discontinuous control; in low-entropy prompts becomes near-greedy, reducing diversity.\n",
      "- Guidance: Prefer temperature for graded control; use top-p to bound “surprise.” Common combo: temperature ~0.7–1.0 with top-p ~0.9–0.95; avoid extremes and tune per task.\n"
     ]
    }
   ],
   "source": [
    "#GPT-5\n",
    "resp = client.responses.create(\n",
    "  model=\"gpt-5\",\n",
    "  input=[\n",
    "    {\"role\":\"system\",\"content\":\"Verbosity: low. Tone: professional. Output as 2–3 bullets.\"},\n",
    "    {\"role\":\"user\",\"content\":\"Summarize the pros and cons of Top-p vs Temperature.\"}\n",
    "  ]\n",
    ")\n",
    "print(resp.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075261af",
   "metadata": {},
   "source": [
    "5) Light reasoning with short rationale (no hidden chain-of-thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3725027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For deterministic outputs, setting temperature=0 is better because it removes randomness by always choosing the highest probability option. In contrast, top_p=1 allows for sampling from the entire probability distribution, which can introduce variability.\n"
     ]
    }
   ],
   "source": [
    "#GPT-4\n",
    "resp = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  temperature=0.2,\n",
    "  messages=[\n",
    "    {\"role\":\"system\",\"content\":\"Answer with a brief rationale (2 sentences max).\"},\n",
    "    {\"role\":\"user\",\"content\":\"Which is better for deterministic outputs: temperature=0 or top_p=1?\"}\n",
    "  ]\n",
    ")\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ad7e4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature=0. It forces greedy/argmax decoding (no sampling), while top_p=1 only disables nucleus filtering and can still be stochastic; for strict determinism also avoid sampling and fix seeds/hardware settings.\n"
     ]
    }
   ],
   "source": [
    "#GPT-5\n",
    "resp = client.responses.create(\n",
    "  model=\"gpt-5\",\n",
    "  input=[\n",
    "    {\"role\":\"system\",\"content\":\"Provide the answer and a brief rationale (≤2 sentences). Verbosity: low.\"},\n",
    "    {\"role\":\"user\",\"content\":\"Which is better for deterministic outputs: temperature=0 or top_p=1?\"}\n",
    "  ]\n",
    ")\n",
    "print(resp.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
