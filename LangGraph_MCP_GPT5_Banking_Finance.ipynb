{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e1d759",
   "metadata": {},
   "source": [
    "# LangGraph + MCP + GPT-5 for Banking and Finance\n",
    "\n",
    "This notebook demonstrates integration of **Model Context Protocol (MCP)** with **LangGraph** and **GPTâ€‘5** to build toolâ€‘calling AI workflows for **Banking & Finance**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97269f98",
   "metadata": {},
   "source": [
    "## ðŸ§© What is Model Context Protocol (MCP)\n",
    "**MCP** is an open standard that defines how Large Language Models communicate with external tools and APIs.\n",
    "\n",
    "- Discover available tools (`list_tools`)\n",
    "- Call tools dynamically (`call_tool`)\n",
    "- Receive structured JSON responses\n",
    "\n",
    "Think of MCP as the **USB protocol for AI tools** â€” plug & play interoperability for any LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca09284c",
   "metadata": {},
   "source": [
    "## ðŸ¤” Why MCP ?\n",
    "| Traditional Integration | With MCP |\n",
    "|--------------------------|----------|\n",
    "| Hard-coded APIs | Dynamic tool discovery |\n",
    "| App-specific contracts | Standardized JSON-RPC |\n",
    "| Tight coupling | Sandboxed isolation |\n",
    "| One LLM per API | Multi-LLM interoperability |\n",
    "\n",
    "**Benefits**\n",
    "- ðŸ”Œ Interoperable\n",
    "- ðŸ”’ Secure and sandboxed\n",
    "- ðŸ§  Auto-discovery of tools\n",
    "- âš™ï¸ Works with LangGraph, Copilot, Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7b1d4",
   "metadata": {},
   "source": [
    "## âš™ï¸ MCP Architecture\n",
    "```\n",
    "GPTâ€‘5 â†” LangGraph â†” MCP Client â†” MCP Server â†” Business Tools\n",
    "```\n",
    "**Server** â†’ hosts logic (e.g., EMI calculator)\n",
    "\n",
    "**Client** â†’ bridges LangGraph â†” Server\n",
    "\n",
    "**LLM Agent** â†’ GPTâ€‘5 decides which tool to call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cddec3",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U mcp langchain-mcp langchain-openai langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71709867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-your-key-here\"  # Replace with valid key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097a5a7",
   "metadata": {},
   "source": [
    "## ðŸ§± Basic MCP Server Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36cccdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_server.py\n",
    "\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"BankingMCP\")\n",
    "\n",
    "@mcp.tool()\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@mcp.tool()\n",
    "def message_of_the_day() -> str:\n",
    "    return \"Stay curious. Build boldly.\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mcp_server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de70be",
   "metadata": {},
   "source": [
    "## ðŸ§ª MCP Client Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea42b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mcp_client_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_client_test.py\n",
    "\n",
    "import asyncio\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "async def test_client():\n",
    "    params = StdioServerParameters(command=\"python\", args=[\"mcp_server.py\"])\n",
    "    async with stdio_client(params) as (r, w):\n",
    "        async with ClientSession(r, w) as session:\n",
    "            await session.initialize()\n",
    "            tools = await session.list_tools()\n",
    "            print(\"Available tools:\", [t[0] for t in tools])\n",
    "            result = await session.call_tool(\"add\", {\"a\": 10, \"b\": 20})\n",
    "            print(\"Result:\", result.structuredContent.get(\"result\"))\n",
    "\n",
    "asyncio.run(test_client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16125b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[10/31/25 22:25:29]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=985938;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=300098;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ListToolsRequest                      \u001b[2m             \u001b[0m\n",
      "Available tools: ['meta', 'nextCursor', 'tools']\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=955175;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=81691;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "Result: 30\n"
     ]
    }
   ],
   "source": [
    "!python mcp_client_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144f283",
   "metadata": {},
   "source": [
    "## ðŸ¤– LangGraph + GPTâ€‘5 Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f44ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langchain_mcp_integration.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langchain_mcp_integration.py\n",
    "\n",
    "import asyncio, warnings\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp import MCPToolkit\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "async def run_agent():\n",
    "    params = StdioServerParameters(command=\"python\", args=[\"mcp_server.py\"])\n",
    "    async with stdio_client(params) as (r, w):\n",
    "        async with ClientSession(r, w) as session:\n",
    "            await session.initialize()\n",
    "            toolkit = MCPToolkit(session=session)\n",
    "            await toolkit.initialize()\n",
    "            tools = toolkit.get_tools()\n",
    "            print(\"Loaded tools:\", [t.name for t in tools])\n",
    "\n",
    "            llm = ChatOpenAI(model=\"gpt-5\", temperature=0, request_timeout=30)\n",
    "            agent = create_react_agent(llm, tools)\n",
    "            result = await agent.ainvoke({\n",
    "                \"messages\":[(\"user\",\"Add 40 and 60 then give message of the day.\")]\n",
    "            })\n",
    "            print(\"ðŸ’¬ Response:\", result[\"messages\"][-1].content)\n",
    "\n",
    "asyncio.run(run_agent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "949c98e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[10/31/25 22:28:07]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=643788;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=884471;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ListToolsRequest                      \u001b[2m             \u001b[0m\n",
      "âœ… Loaded tools: ['add', 'message_of_the_day']\n",
      "\u001b[2;36m[10/31/25 22:28:17]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=8500;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=875747;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=699554;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=376926;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "ðŸ’¬ Response: Sum: 100\n",
      "Message of the day: Stay curious. Build boldly.\n"
     ]
    }
   ],
   "source": [
    "!python langchain_mcp_integration.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2113273",
   "metadata": {},
   "source": [
    "## ðŸ’° Demoâ€¯1â€¯â€”â€¯Loanâ€¯EMIâ€¯Calculatorâ€¯(MCPâ€¯Server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe4b4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mcp_server_emi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_server_emi.py\n",
    "import math\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "mcp = FastMCP(\"LoanEMIServer\")\n",
    "\n",
    "@mcp.tool()\n",
    "def calculate_emi(principal: float, rate: float, tenure_years: float) -> float:\n",
    "    \"\"\"Calculate monthly EMI.\"\"\"\n",
    "    r = rate / (12 * 100)\n",
    "    n = tenure_years * 12\n",
    "    emi = principal * r * ((1 + r)**n) / ((1 + r)**n - 1)\n",
    "    return round(emi, 2)\n",
    "\n",
    "@mcp.tool()\n",
    "def message_of_the_day() -> str:\n",
    "    return \"Dream big â€” finance smartly ðŸ’¡\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d163336",
   "metadata": {},
   "source": [
    "### ðŸ§® EMIâ€¯Clientâ€¯+â€¯LangGraphâ€¯Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "983fced0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mcp_client_emi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_client_emi.py\n",
    "\n",
    "import asyncio\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp import MCPToolkit\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "async def run_emi_agent():\n",
    "    params = StdioServerParameters(command=\"python\", args=[\"mcp_server_emi.py\"])\n",
    "    async with stdio_client(params) as (r, w):\n",
    "        async with ClientSession(r, w) as session:\n",
    "            await session.initialize()\n",
    "            toolkit = MCPToolkit(session=session)\n",
    "            await toolkit.initialize()\n",
    "            tools = toolkit.get_tools()\n",
    "            print(\"Tools:\", [t.name for t in tools])\n",
    "\n",
    "            llm = ChatOpenAI(model=\"gpt-5\", temperature=0)\n",
    "            agent = create_react_agent(llm, tools)\n",
    "            result = await agent.ainvoke({\n",
    "                \"messages\":[(\"user\",\"Calculate EMI for â‚¹5â€¯L loanâ€¯@â€¯10%â€¯forâ€¯3â€¯years and share message of the day.\")]\n",
    "            })\n",
    "            print(\"Agent Response:\", result[\"messages\"][-1].content)\n",
    "\n",
    "asyncio.run(run_emi_agent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "665a61de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[10/31/25 22:30:27]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=119933;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=970310;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ListToolsRequest                      \u001b[2m             \u001b[0m\n",
      "Tools: ['calculate_emi', 'message_of_the_day']\n",
      "\u001b[2;36m[10/31/25 22:30:33]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=453181;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=840959;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=98476;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=578;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "Agent Response: Here you go:\n",
      "\n",
      "- EMI for â‚¹5,00,000 at 10% for 3 years: â‚¹16,133.59 per month\n",
      "- Message of the Day: Dream big â€” finance smartly ðŸ’¡\n"
     ]
    }
   ],
   "source": [
    "!python mcp_client_emi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a79ceab",
   "metadata": {},
   "source": [
    "## ðŸ§® Demoâ€¯2â€¯â€”â€¯Riskâ€¯Scoringâ€¯Toolâ€¯(MCPâ€¯Server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e756a202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mcp_server_risk.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_server_risk.py\n",
    "\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "mcp = FastMCP(\"RiskServer\")\n",
    "\n",
    "@mcp.tool()\n",
    "def calculate_risk_score(age: int, income: float, liabilities: float) -> str:\n",
    "    \"\"\"Return basic financial risk score.\"\"\"\n",
    "    ratio = liabilities / income if income else 1\n",
    "    score = max(0, 100 - (age*0.2 + ratio*50))\n",
    "    if score > 75: return \"Low Risk\"\n",
    "    elif score > 40: return \"Moderate Risk\"\n",
    "    else: return \"High Risk\"\n",
    "\n",
    "@mcp.tool()\n",
    "def message_of_the_day() -> str:\n",
    "    return \"Plan your finances before they plan you ðŸ’¼\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691e9743",
   "metadata": {},
   "source": [
    "### ðŸ§  Riskâ€¯Scoringâ€¯Clientâ€¯+â€¯LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1896549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mcp_client_risk.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_client_risk.py\n",
    "\n",
    "import asyncio\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp import MCPToolkit\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "async def run_risk_agent():\n",
    "    params = StdioServerParameters(command=\"python\", args=[\"mcp_server_risk.py\"])\n",
    "    async with stdio_client(params) as (r, w):\n",
    "        async with ClientSession(r, w) as session:\n",
    "            await session.initialize()\n",
    "            toolkit = MCPToolkit(session=session)\n",
    "            await toolkit.initialize()\n",
    "            tools = toolkit.get_tools()\n",
    "            print(\"Tools:\", [t.name for t in tools])\n",
    "\n",
    "            llm = ChatOpenAI(model=\"gpt-5\", temperature=0)\n",
    "            agent = create_react_agent(llm, tools)\n",
    "            result = await agent.ainvoke({\n",
    "                \"messages\":[(\"user\",\"Calculate risk score for ageâ€¯=â€¯35, incomeâ€¯=â€¯15â€¯Lâ€¯INR, liabilitiesâ€¯=â€¯3â€¯Lâ€¯INR.\")]\n",
    "            })\n",
    "            print(\"Agent Response:\", result[\"messages\"][-1].content)\n",
    "\n",
    "asyncio.run(run_risk_agent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f89c943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[10/31/25 22:32:06]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=139469;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=63451;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ListToolsRequest                      \u001b[2m             \u001b[0m\n",
      "Tools: ['calculate_risk_score', 'message_of_the_day']\n",
      "\u001b[2;36m[10/31/25 22:32:11]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=300616;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=487186;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "Agent Response: Your financial risk score is: Low Risk.\n",
      "\n",
      "Inputs considered:\n",
      "- Age: 35\n",
      "- Income: â‚¹15,00,000 per year\n",
      "- Liabilities: â‚¹3,00,000\n",
      "\n",
      "If youâ€™d like, I can break down how this score is calculated or help you see how changes (e.g., higher liabilities or lower income) would affect it.\n"
     ]
    }
   ],
   "source": [
    "!python mcp_client_risk.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8cc5f",
   "metadata": {},
   "source": [
    "## ðŸ”§ Futureâ€¯Demosâ€¯(Placeholders)\n",
    "- Accountâ€¯Balanceâ€¯Calculator\n",
    "- Creditâ€¯Cardâ€¯Rewardâ€¯Estimator\n",
    "- Transactionâ€¯Analyzerâ€¯(Alerts)\n",
    "- Naturalâ€¯Languageâ€¯Bankingâ€¯Queryâ€¯Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da60800",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "- MCPâ€¯standardizesâ€¯toolâ€¯callingâ€¯betweenâ€¯LLMsâ€¯andâ€¯code.\n",
    "- LangGraphâ€¯addsâ€¯statefulâ€¯multiâ€‘toolâ€¯reasoning.\n",
    "- GPTâ€‘5â€¯actsâ€¯asâ€¯theâ€¯controllerâ€¯toâ€¯invokeâ€¯bankingâ€¯toolsâ€¯intelligently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e26c5",
   "metadata": {},
   "source": [
    "## ðŸ“š References\n",
    "- [ModelContextProtocol.io](https://modelcontextprotocol.io)\n",
    "- [LangGraphâ€¯Docs](https://langchain-ai.github.io/langgraph/)\n",
    "- [LangChainâ€¯MCPâ€¯GitHub](https://github.com/rectalogic/langchain-mcp)\n",
    "- [OpenAIâ€¯GPTâ€‘5â€¯Docs](https://platform.openai.com/docs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
