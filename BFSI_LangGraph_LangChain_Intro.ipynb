{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56b48e5",
   "metadata": {},
   "source": [
    "\n",
    "## LangGraph + LangChain\n",
    "\n",
    "This hands-on notebook introduces **LangGraph** and **LangChain (OpenAI)** for agentic AI in Banking/Finance:\n",
    "- **Intro to LangGraph**\n",
    "- **LangGraph basic example (typed state)**  \n",
    "- **LangGraph with GPT (via `langchain-openai`)** *(model name configurable; use `gpt-5` if your account has access)*\n",
    "- **Use case: LangChain + LangGraph + GPT â€” banking FAQ summarizer**\n",
    "- **Multi-Agent Orchestration:** coordinating agents, context passing, decision routing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d507094b",
   "metadata": {},
   "source": [
    "## LangGraph \n",
    "\n",
    "LangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e72822",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup & Versions\n",
    "Install/upgrade required packages. If your org pins versions, adapt accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32504945",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langgraph langchain langchain-core langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05eeaaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:49:36) [Clang 16.0.6 ]\n",
      "Platform: macOS-26.1-arm64-arm-64bit\n",
      "Loaded .env\n",
      "OPENAI_API_KEY set: True\n",
      "OPENAI_MODEL: gpt-5\n"
     ]
    }
   ],
   "source": [
    "# If needed, uncomment to install/upgrade.\n",
    "# %pip install -U langgraph langchain langchain-core langchain-openai python-dotenv\n",
    "\n",
    "import sys, platform, os\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "# Optional: load a .env if present for OPENAI_API_KEY and OPENAI_MODEL\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"Loaded .env\")\n",
    "except Exception as e:\n",
    "    print(\"python-dotenv not installed or .env absent (ok).\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5\")  # set to \"gpt-5\" if your account has access\n",
    "print(\"OPENAI_API_KEY set:\", bool(OPENAI_API_KEY))\n",
    "print(\"OPENAI_MODEL:\", OPENAI_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb3f4d4",
   "metadata": {},
   "source": [
    "\n",
    "## 1) LangGraph â€” What & Why (Quick Intro)\n",
    "\n",
    "**LangGraph** helps you build **agentic workflows** \n",
    "as a graph of nodes with explicit **state**:\n",
    "\n",
    "- **StateGraph**: define a typed state (keys your nodes read/write).\n",
    "\n",
    "- **Nodes**: pure functions (sync or async) that transform state.\n",
    "\n",
    "- **Edges**: connect nodes; use **conditional edges** for **decision routing**.\n",
    "\n",
    "- **START / END**: special markers to begin and end execution.\n",
    "\n",
    "This explicit structure is ideal for BFSI where **auditable flows**, **guardrails**, and **determinism** matter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d092368",
   "metadata": {},
   "source": [
    "\n",
    "## 2) LangGraph Basic Example â€” Typed State, Nodes, Edges\n",
    "\n",
    "We implement a simple **KYC guard â†’ Retriever â†’ Summarizer** flow.\n",
    "- If the query contains sensitive hints (e.g., `password`, `otp`, `pin`), we **block**.\n",
    "- Else we **retrieve** (stub) and **summarize** (stub).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "74b4970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAFE PATH => {'query': 'Latest retail loan policy', 'route': 'process', 'docs': ['Policy doc snippet for: Latest retail loan policy', 'Interest rate depends on tenure and credit score; see policy 2024-09.'], 'summary': 'Summary: Policy doc snippet for: Latest retail loan policy | Interest rate depends on tenure and credit score; see policy 2024-09.'}\n",
      "BLOCK PATH => {'query': 'What is my OTP right now?', 'route': 'block', 'warning': 'ðŸš« Sensitive/PII-like term detected. Request denied.'}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "class BFState(TypedDict, total=False):\n",
    "    query: str\n",
    "    route: Literal[\"block\", \"process\"]\n",
    "    docs: list[str]\n",
    "    summary: str\n",
    "    warning: str\n",
    "\n",
    "RISK_WORDS = {\"password\", \"otp\", \"pin\", \"cvv\", \"secret\"}\n",
    "\n",
    "def kyc_guard(state: BFState) -> BFState:\n",
    "    q = state[\"query\"].lower()\n",
    "    if any(w in q for w in RISK_WORDS):\n",
    "        return {\"route\": \"block\", \"warning\": \"ðŸš« Sensitive/PII-like term detected. Request denied.\"}\n",
    "    return {\"route\": \"process\"}\n",
    "\n",
    "def retriever(state: BFState) -> BFState:\n",
    "    q = state[\"query\"]\n",
    "    # In production: query vector DB (Chroma/pgvector) or search service\n",
    "    return {\"docs\": [f\"Policy doc snippet for: {q}\",\n",
    "                     \"Interest rate depends on tenure and credit score; see policy 2024-09.\"]}\n",
    "\n",
    "def summarizer(state: BFState) -> BFState:\n",
    "    docs = state.get(\"docs\", [])\n",
    "    if not docs:\n",
    "        return {\"summary\": \"No data.\"}\n",
    "    return {\"summary\": \"Summary: \" + \" | \".join(docs)}\n",
    "\n",
    "graph = StateGraph(BFState)\n",
    "graph.add_node(\"kyc_guard\", kyc_guard)\n",
    "graph.add_node(\"retriever\", retriever)\n",
    "graph.add_node(\"summarizer\", summarizer)\n",
    "\n",
    "graph.add_edge(START, \"kyc_guard\")\n",
    "graph.add_conditional_edges(\n",
    "    \"kyc_guard\",\n",
    "    lambda s: s[\"route\"],\n",
    "    {\"block\": END, \"process\": \"retriever\"}\n",
    ")\n",
    "graph.add_edge(\"retriever\", \"summarizer\")\n",
    "graph.add_edge(\"summarizer\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "print(\"SAFE PATH =>\", app.invoke({\"query\": \"Latest retail loan policy\"}))\n",
    "print(\"BLOCK PATH =>\", app.invoke({\"query\": \"What is my OTP right now?\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0b813",
   "metadata": {},
   "source": [
    "\n",
    "### 3) LangGraph + GPT (via `langchain-openai`)\n",
    "\n",
    "We'll add an **LLM summarizer** node powered by OpenAI.  \n",
    "> Set `OPENAI_API_KEY` and choose a model via `OPENAI_MODEL`. If your workspace has **GPTâ€‘5**, set `OPENAI_MODEL=\"gpt-5\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a7e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM enabled: True\n",
      "{'query': 'Summarize retail loan policy changes', 'docs': ['Policy doc snippet for: Summarize retail loan policy changes', 'Risk weights revised in RBI circular X.'], 'llm_summary': '**Summary for Banking Analyst:**\\n\\n1. **Retail Loan Policy Changes:**\\n   - The policy document outlines recent amendments to the retail loan segment. Key changes include adjustments in eligibility criteria, interest rate structures, and loan-to-value (LTV) ratios. The policy aims to enhance credit accessibility while maintaining prudent risk management. Additionally, there is a focus on digital lending processes to streamline application and approval workflows, improving customer experience and operational efficiency.\\n\\n2. **RBI Circular X - Revised Risk Weights:**\\n   - The Reserve Bank of India (RBI) has issued Circular X, which revises the risk weights applicable to various asset classes. The changes are intended to align with international standards and ensure a more robust risk management framework. Notably, risk weights for unsecured retail loans have been increased, reflecting heightened risk perceptions. Conversely, risk weights for secured loans, such as those backed by residential mortgages, have been slightly reduced, acknowledging their lower risk profile. These adjustments are expected to impact capital allocation and pricing strategies within banks.'}\n"
     ]
    }
   ],
   "source": [
    "USE_LLM = bool(OPENAI_API_KEY)\n",
    "print(\"LLM enabled:\", USE_LLM)\n",
    "\n",
    "if USE_LLM:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.messages import HumanMessage\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "\n",
    "    class LLMState(TypedDict, total=False):\n",
    "        query: str\n",
    "        docs: list[str]\n",
    "        llm_summary: str\n",
    "\n",
    "    async def llm_summarizer(state: LLMState) -> LLMState:\n",
    "        docs = state.get(\"docs\") or []\n",
    "        prompt_text = \"Summarize for a banking analyst:\\n\" + \"\\n\".join(f\"- {d}\" for d in docs) or \"No docs.\"\n",
    "        resp = await llm.ainvoke([HumanMessage(content=prompt_text)])\n",
    "        return {\"llm_summary\": resp.content}\n",
    "\n",
    "    from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "    g_llm = StateGraph(LLMState)\n",
    "    # reuse retriever from above for demo\n",
    "    def retriever_llm(state: LLMState) -> LLMState:\n",
    "        q = state[\"query\"]\n",
    "        return {\"docs\": [f\"Policy doc snippet for: {q}\", \"Risk weights revised in RBI circular X.\"]}\n",
    "\n",
    "    g_llm.add_node(\"retriever\", retriever_llm)\n",
    "    g_llm.add_node(\"llm_summarizer\", llm_summarizer)\n",
    "    g_llm.add_edge(START, \"retriever\")\n",
    "    g_llm.add_edge(\"retriever\", \"llm_summarizer\")\n",
    "    g_llm.add_edge(\"llm_summarizer\", END)\n",
    "\n",
    "    app_llm = g_llm.compile()\n",
    "\n",
    "\n",
    "    import nest_asyncio, asyncio\n",
    "    nest_asyncio.apply()\n",
    "    out = await app_llm.ainvoke({\"query\": \"Summarize retail loan policy changes\"})\n",
    "    print(out[\"\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d4b5c7",
   "metadata": {},
   "source": [
    "Script/terminal version (if you move to a .py file)\n",
    "\n",
    "    Outside Jupyter (plain Python script), keep asyncio.run(...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    out = asyncio.run(app_llm.ainvoke({\"query\": \"Summarize retail loan policy changes\"}))\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be7f588",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Use Case: LangChain + LangGraph + GPT â€” Banking FAQ Summarizer\n",
    "\n",
    "Flow:\n",
    "1. Guard â†’ 2. Retrieve FAQs â†’ 3. LLM summarize â†’ 4. Return concise answer\n",
    "\n",
    "This pattern fits **contact center assistants**, **ops copilots**, or **policy desk bots**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3a5a0d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting faq_app_langgraph.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile faq_app_langgraph.py\n",
    "\n",
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5\")  # set to \"gpt-5\" if your account has access\n",
    "print(\"OPENAI_API_KEY set:\", bool(OPENAI_API_KEY))\n",
    "print(\"OPENAI_MODEL:\", OPENAI_MODEL)\n",
    "\n",
    "class FAQState(TypedDict, total=False):\n",
    "    question: str\n",
    "    route: Literal[\"block\", \"ok\"]\n",
    "    faqs: list[str]\n",
    "    answer: str\n",
    "    reason: str\n",
    "\n",
    "def guard(state: FAQState) -> FAQState:\n",
    "    q = state[\"question\"].lower()\n",
    "    if any(x in q for x in [\"password\", \"otp\", \"cvv\", \"pin\", \"secret\"]):\n",
    "        return {\"route\": \"block\", \"reason\": \"ðŸš« Sensitive query blocked.\"}\n",
    "    return {\"route\": \"ok\"}\n",
    "\n",
    "def retrieve_faqs(state: FAQState) -> FAQState:\n",
    "    q = state[\"question\"]\n",
    "    # Stub: In production, fetch from indexed policies / FAQ DB\n",
    "    return {\"faqs\": [f\"FAQ: Response policy related to: {q}\",\n",
    "                     \"Customers must not share OTP/CVV; see security policy 2025-A.\"]}\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.messages import HumanMessage\n",
    "    llm2 = ChatOpenAI(model=OPENAI_MODEL, temperature=0)\n",
    "\n",
    "    async def answer_llm(state: FAQState) -> FAQState:\n",
    "        faqs = state.get(\"faqs\") or []\n",
    "        prompt = \"Create a concise, compliant answer for a banking customer:\\n\" + \"\\n\".join(f\"- {f}\" for f in faqs)\n",
    "        resp = await llm2.ainvoke([HumanMessage(content=prompt)])\n",
    "        return {\"answer\": resp.content}\n",
    "else:\n",
    "    def answer_llm(state: FAQState) -> FAQState:\n",
    "        faqs = state.get(\"faqs\") or []\n",
    "        return {\"answer\": \"LLM disabled. Heuristic answer: \" + \" | \".join(faqs)}\n",
    "\n",
    "faq_graph = StateGraph(FAQState)\n",
    "faq_graph.add_node(\"guard\", guard)\n",
    "faq_graph.add_node(\"retrieve_faqs\", retrieve_faqs)\n",
    "faq_graph.add_node(\"answer_llm\", answer_llm)\n",
    "\n",
    "faq_graph.add_edge(START, \"guard\")\n",
    "faq_graph.add_conditional_edges(\n",
    "    \"guard\",\n",
    "    lambda s: s[\"route\"],\n",
    "    {\"block\": END, \"ok\": \"retrieve_faqs\"}\n",
    ")\n",
    "faq_graph.add_edge(\"retrieve_faqs\", \"answer_llm\")\n",
    "faq_graph.add_edge(\"answer_llm\", END)\n",
    "\n",
    "faq_app = faq_graph.compile()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    out = asyncio.run(faq_app.ainvoke({\"question\": \"What is the current FD interest rate for 1 year?\"}))\n",
    "    #print(\"FAQ SAFE PATH =>\", out)\n",
    "    print(out['question'], \"=>\", out.get('answer'))\n",
    "\n",
    "    out1 = asyncio.run(faq_app.ainvoke({\"question\": \"Share my OTP please\"}))\n",
    "    print(\"FAQ BLOCK PATH =>\", out1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "68df3f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY set: True\n",
      "OPENAI_MODEL: gpt-5\n",
      "What is the current FD interest rate for 1 year? => FAQ: What is the current FD interest rate for 1 year?\n",
      "\n",
      "FD rates are dynamic and may change without notice. For the latest 1â€‘year FD rate applicable to your profile:\n",
      "- Visit our official website (Interest Rates > Fixed Deposits)\n",
      "- Check our mobile app/NetBanking (Open/Book FD to view live rates for your amount and tenor)\n",
      "- Visit your branch or call our official helpline\n",
      "\n",
      "Note: Rates can vary by customer category (e.g., senior citizen), deposit amount, tenor, and payout option.\n",
      "\n",
      "Security reminder (Policy 2025-A): Never share your OTP, CVV, card/PIN, or passwords. We will never ask for these over calls, emails, chat, or links. If unsure, contact us via official channels only.\n",
      "FAQ BLOCK PATH => {'question': 'Share my OTP please', 'route': 'block', 'reason': 'ðŸš« Sensitive query blocked.'}\n"
     ]
    }
   ],
   "source": [
    "!python faq_app_langgraph.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec74fb7",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Multiâ€‘Agent Orchestration â€” Coordinating Agents, Context Passing, Decision Routing\n",
    "\n",
    "We build a small **router** that sends queries to one of two agents:\n",
    "- **Calc Agent**: safely evaluates basic arithmetic (no `eval`).\n",
    "- **Policy Agent**: returns policy-search stubs (replace with vector DB later).\n",
    "\n",
    "**Context Passing**: both agents read from the same typed state; router sets a `route` key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "35937258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum 18 + 24 => 42\n",
      "policy for retail loans under 25L? => ðŸ”Ž (Stub) Policy search result for: policy for retail loans under 25L?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ast, operator as op\n",
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "OPS = {ast.Add: op.add, ast.Sub: op.sub, ast.Mult: op.mul, ast.Div: op.truediv}\n",
    "\n",
    "class RouteState(TypedDict, total=False):\n",
    "    query: str\n",
    "    route: Literal[\"calc_agent\", \"policy_agent\"]\n",
    "    result: str\n",
    "\n",
    "def router(state: RouteState) -> RouteState:\n",
    "    q = state[\"query\"].lower()\n",
    "    if any(tok in q for tok in [\"+\", \"-\", \"*\", \"/\", \"sum\", \"calculate\"]):\n",
    "        return {\"route\": \"calc_agent\"}\n",
    "    return {\"route\": \"policy_agent\"}\n",
    "\n",
    "def calc_agent(state: RouteState) -> RouteState:\n",
    "    text = state[\"query\"].lower().replace(\"sum\", \"\").replace(\"calculate\", \"\").strip()\n",
    "    try:\n",
    "        node = ast.parse(text, mode=\"eval\").body\n",
    "        def _eval(n):\n",
    "            if isinstance(n, ast.Num):  # type: ignore[attr-defined]\n",
    "                return float(n.n)\n",
    "            if isinstance(n, ast.BinOp) and type(n.op) in OPS:\n",
    "                return OPS[type(n.op)](_eval(n.left), _eval(n.right))\n",
    "            raise ValueError(\"Only + - * / supported.\")\n",
    "        val = _eval(node)\n",
    "        return {\"result\": f\"{val:g}\"}\n",
    "    except Exception:\n",
    "        return {\"result\": \"Failed to parse expression.\"}\n",
    "\n",
    "def policy_agent(state: RouteState) -> RouteState:\n",
    "    q = state[\"query\"]\n",
    "    return {\"result\": f\"ðŸ”Ž (Stub) Policy search result for: {q}\"}\n",
    "\n",
    "rg = StateGraph(RouteState)\n",
    "rg.add_node(\"router\", router)\n",
    "rg.add_node(\"calc_agent\", calc_agent)\n",
    "rg.add_node(\"policy_agent\", policy_agent)\n",
    "\n",
    "rg.add_edge(START, \"router\")\n",
    "rg.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda s: s[\"route\"],\n",
    "    {\"calc_agent\": \"calc_agent\", \"policy_agent\": \"policy_agent\"}\n",
    ")\n",
    "rg.add_edge(\"calc_agent\", END)\n",
    "rg.add_edge(\"policy_agent\", END)\n",
    "\n",
    "route_app = rg.compile()\n",
    "\n",
    "import asyncio\n",
    "\n",
    "out= asyncio.run(route_app.ainvoke({\"query\": \"sum 18 + 24\"}))\n",
    "print(out[\"query\"], \"=>\", out[\"result\"])\n",
    "\n",
    "out1 = asyncio.run(route_app.ainvoke({\"query\": \"policy for retail loans under 25L?\"}))\n",
    "#print(out1)\n",
    "print(out1[\"query\"], \"=>\", out1[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
